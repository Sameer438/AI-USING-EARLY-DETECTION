{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl5DkKg/xnBLf0VC7pTsUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer438/AI-USING-EARLY-DETECTION/blob/main/Tubercolis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary libraries:\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "metadata": {
        "id": "2noaQyho1ccH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tawsifurrahman/tuberculosis-tb-chest-xray-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as0DzCsF1a5t",
        "outputId": "82448d36-8b1e-4c4d-cc22-5cca7ee9dcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset/versions/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the values needed for all the image files\n",
        "normaldir = '/kaggle/input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database/Normal'\n",
        "tbdir = '/kaggle/input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database/Tuberculosis'\n",
        "images = []\n",
        "labels = []\n",
        "imagesize = 256"
      ],
      "metadata": {
        "id": "eG6ORiHR1g3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tawsifurrahman/tuberculosis-tb-chest-xray-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "#Initializing the values needed for all the image files\n",
        "# Update the paths to reflect the actual location of the dataset\n",
        "# You might need to inspect the 'path' variable to determine the correct location\n",
        "normaldir = os.path.join(path, 'TB_Chest_Radiography_Database', 'Normal')\n",
        "tbdir = os.path.join(path, 'TB_Chest_Radiography_Database', 'Tuberculosis')\n",
        "images = []\n",
        "labels = []\n",
        "imagesize = 256\n",
        "\n",
        "#Storing all the image directories in the 'images' array and corresponding them to either 1 for TB images or 0 for normal images.\n",
        "for x in os.listdir(normaldir):\n",
        "    imagedir = os.path.join(normaldir, x)\n",
        "    image = cv.imread(imagedir, cv.IMREAD_GRAYSCALE)\n",
        "    image = cv.resize(image, (imagesize, imagesize))\n",
        "    images.append(image)\n",
        "    labels.append(0)\n",
        "\n",
        "for y in os.listdir(tbdir):\n",
        "    imagedir = os.path.join(tbdir, y)\n",
        "    image = cv.imread(imagedir, cv.IMREAD_GRAYSCALE)\n",
        "    image = cv.resize(image, (imagesize, imagesize))\n",
        "    images.append(image)\n",
        "    labels.append(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8eFUwpl10Ys",
        "outputId": "e94b9d73-504f-4b8b-9307-b3a0aa93ae6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset/versions/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting to NumPy arrays since they have more features than regular lists\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "#Splitting the images and labels into training and testing sets, then normalizing the values within them for computational efficiency (from 0-255 scale to 0-1 scale)\n",
        "imagetrain, imagetest, labeltrain, labeltest = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
        "imagetrain = (imagetrain.astype('float32'))/255\n",
        "imagetest = (imagetest.astype('float32'))/255"
      ],
      "metadata": {
        "id": "DPcKdMCa2BSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flattening the image array into 2D (making it [2940 images] x [all the pixels of the image in just one 1D array]) to be suitable for SMOTE oversampling\n",
        "imagetrain = imagetrain.reshape(2940, (imagesize*imagesize))\n",
        "\n",
        "#Performing oversampling\n",
        "smote = SMOTE(random_state=42)\n",
        "imagetrain, labeltrain = smote.fit_resample(imagetrain, labeltrain)\n",
        "\n",
        "#Unflattening the images now to use them for convolutional neural network (4914 images of 256x256 size, with 1 color channel (grayscale, as compared to RGB with 3 color channels))\n",
        "imagetrain = imagetrain.reshape(-1, imagesize, imagesize, 1)\n",
        "print(imagetrain.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLv5lWKO2FUl",
        "outputId": "4828c5b3-009f-4ff9-a54c-2761a3c2abcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4914, 256, 256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classes balanced - equal counts of each label\n",
        "print(np.unique(labeltrain, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slG8Wyc42Jaq",
        "outputId": "6eb77abc-42e2-4a4e-cfb4-d3cfb9c3fbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0, 1]), array([2457, 2457]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "3foF75SF2Mp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The CNN model has 3 convolutional layers, each followed by pooling to summarize the features found by the layer, starting with 16 and multiplying by 2 each time for computational efficiency, as bits are structured in powers of 2. 3x3 filters and ReLU activation used.\n",
        "cnn = keras.Sequential(\n",
        "    [\n",
        "    #Input layer, same shape as all the images (256x256x1):\n",
        "    keras.Input(shape=(imagesize, imagesize, 1)),\n",
        "\n",
        "    #1st convolutional layer:\n",
        "    Conv2D(16, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    #2nd convolutional layer:\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    #3rd convolutional layer:\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    #Flattening layer for the dense layers:\n",
        "    Flatten(),\n",
        "\n",
        "    #1st dense layer following the convolutional layers:\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    #Dropout layer with heavy dropout rate to avoid overfitting in the large-ish dataset\n",
        "    Dropout(0.5),\n",
        "\n",
        "    #Output layer that squeezes each image to either 0 or 1 with sigmoid activation\n",
        "    Dense(1, activation='sigmoid')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Asfl4lFR2NXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model with parameters best suited for the task at hand:\n",
        "cnn.compile(\n",
        "    loss='binary_crossentropy', #Best for binary classification\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001), #Good starting LR for dataset of this size\n",
        "    metrics=['accuracy'], #Looking for accuracy\n",
        ")"
      ],
      "metadata": {
        "id": "3d_Se__B2Qsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model, with the ReduceLROnPlateau callback added to it to reduce the learning rate to take smaller steps in increasing the accuracy whenever the learning rate plateaus (goes in the wrong direction)\n",
        "#Doing this with patience=1, meaning it will perform this if it even plateaus for one epoch, since only 10 epochs are used\n",
        "#factor=0.1 means that for every time the learning rate is reduced, it is reduced by a factor of 0.1 - it also won't go lower than 0.00001\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=1, min_lr=0.00001, verbose=1)\n",
        "\n",
        "#Fitting the model w/ the callback. ON VS CODE, batch size of 16 makes each epoch take around a minute in this case w/ good accuracy, making the whole training process 10 min, but on Kaggle it should take longer due to less computational resources:\n",
        "cnn.fit(imagetrain, labeltrain, batch_size=16, epochs=10, verbose=2, callbacks = [reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94zSl8rY2Tx2",
        "outputId": "b1d6d754-1678-4399-e9db-dba986460032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "308/308 - 307s - 998ms/step - accuracy: 0.8555 - loss: 0.3403 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "308/308 - 304s - 987ms/step - accuracy: 0.9501 - loss: 0.1397 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "308/308 - 323s - 1s/step - accuracy: 0.9742 - loss: 0.0805 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "308/308 - 289s - 939ms/step - accuracy: 0.9803 - loss: 0.0650 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "308/308 - 285s - 924ms/step - accuracy: 0.9847 - loss: 0.0437 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "308/308 - 328s - 1s/step - accuracy: 0.9823 - loss: 0.0531 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "308/308 - 316s - 1s/step - accuracy: 0.9912 - loss: 0.0275 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "308/308 - 327s - 1s/step - accuracy: 0.9955 - loss: 0.0156 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "308/308 - 317s - 1s/step - accuracy: 0.9949 - loss: 0.0136 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "308/308 - 286s - 928ms/step - accuracy: 0.9978 - loss: 0.0098 - learning_rate: 1.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ea2604abdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the data w/ multiple types of metrics\n",
        "print('TESTING DATA:')\n",
        "cnn.evaluate(imagetest, labeltest, batch_size=32, verbose=2)\n",
        "\n",
        "print('ADVANCED TESTING METRICS:')\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "predictions = cnn.predict(imagetest, batch_size=32)\n",
        "predicted_labels = (predictions > 0.5).astype('int32')\n",
        "print(classification_report(labeltest, predicted_labels))\n",
        "print(confusion_matrix(labeltest, predicted_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WEflCvf2agf",
        "outputId": "1faf8ddd-4e86-4687-faec-d97579e009b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING DATA:\n",
            "40/40 - 23s - 563ms/step - accuracy: 0.9881 - loss: 0.1021\n",
            "ADVANCED TESTING METRICS:\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 583ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1043\n",
            "           1       0.96      0.97      0.97       217\n",
            "\n",
            "    accuracy                           0.99      1260\n",
            "   macro avg       0.98      0.98      0.98      1260\n",
            "weighted avg       0.99      0.99      0.99      1260\n",
            "\n",
            "[[1035    8]\n",
            " [   7  210]]\n"
          ]
        }
      ]
    }
  ]
}